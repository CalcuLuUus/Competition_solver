{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f338d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. 数据加载与全局特征预计算 (优化) ---\n",
    "print(\"--- 1. Loading and Pre-calculating Global Features ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# 加载数据\n",
    "try:\n",
    "    train_df = pd.read_csv('data/Antai_hackathon_train.csv', parse_dates=['create_order_time'])\n",
    "    test_df = pd.read_csv('data/dianshang_test.csv', parse_dates=['create_order_time'])\n",
    "    item_attr_df = pd.read_csv('data/Antai_hackathon_attr.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"错误：请确保 train.csv, test_without_last.csv, item_attr.csv 文件在当前目录下！\")\n",
    "    exit()\n",
    "\n",
    "# 合并商品属性\n",
    "train_df = pd.merge(train_df, item_attr_df, on='item_id', how='left')\n",
    "test_df = pd.merge(test_df, item_attr_df, on='item_id', how='left')\n",
    "\n",
    "# 时间转换\n",
    "train_df['create_order_time'] = pd.to_datetime(train_df['create_order_time'])\n",
    "test_df['create_order_time'] = pd.to_datetime(test_df['create_order_time'])\n",
    "\n",
    "# 按时间排序，为后续特征工程做准备\n",
    "train_df = train_df.sort_values(by=['buyer_admin_id', 'create_order_time'])\n",
    "test_df = test_df.sort_values(by=['buyer_admin_id', 'create_order_time'])\n",
    "\n",
    "\n",
    "# a. 商品侧特征预计算\n",
    "print(\"Pre-calculating item features...\")\n",
    "item_features = item_attr_df.copy()\n",
    "item_pop = train_df['item_id'].value_counts().reset_index()\n",
    "item_pop.columns = ['item_id', 'item_pop']\n",
    "cate_pop = train_df['cate_id'].value_counts().reset_index()\n",
    "cate_pop.columns = ['cate_id', 'cate_pop']\n",
    "store_pop = train_df['store_id'].value_counts().reset_index()\n",
    "store_pop.columns = ['store_id', 'store_pop']\n",
    "\n",
    "item_features = pd.merge(item_features, item_pop, on='item_id', how='left')\n",
    "item_features = pd.merge(item_features, cate_pop, on='cate_id', how='left')\n",
    "item_features = pd.merge(item_features, store_pop, on='store_id', how='left')\n",
    "item_features.fillna(0, inplace=True)\n",
    "item_features.set_index('item_id', inplace=True) # 使用索引以极大加速后续合并\n",
    "\n",
    "# b. 用户侧特征预计算\n",
    "print(\"Pre-calculating user features...\")\n",
    "user_features = train_df.groupby('buyer_admin_id').agg(\n",
    "    user_total_purchases=('item_id', 'count'),\n",
    "    user_unique_items=('item_id', 'nunique'),\n",
    "    user_unique_cates=('cate_id', 'nunique'),\n",
    "    user_avg_price=('item_price', 'mean')\n",
    ").reset_index()\n",
    "user_features.set_index('buyer_admin_id', inplace=True) # 使用索引\n",
    "\n",
    "# c. 序列Item-CF召回模型 (保持不变，这部分效率尚可)\n",
    "print(\"Building ItemCF recall model...\")\n",
    "item_cf_dict = defaultdict(lambda: defaultdict(int))\n",
    "train_df = train_df.sort_values(by=['buyer_admin_id', 'create_order_time'])\n",
    "for user_id, user_group in tqdm(train_df.groupby('buyer_admin_id'), desc=\"Building ItemCF\"):\n",
    "    item_sequence = user_group['item_id'].tolist()\n",
    "    for i in range(len(item_sequence) - 1):\n",
    "        item_cf_dict[item_sequence[i]][item_sequence[i+1]] += 1\n",
    "\n",
    "global_top_items = train_df['item_id'].value_counts().head(200).index.tolist()\n",
    "print(f\"Preprocessing finished in {(time.time() - start_time)/60:.2f} minutes.\")\n",
    "\n",
    "\n",
    "# --- 2. 召回与训练集构建 (向量化重构) ---\n",
    "print(\"\\n--- 2. Building Training Set (Vectorized) ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# a. 划分训练/验证集的用户\n",
    "all_train_users = train_df['buyer_admin_id'].unique()\n",
    "np.random.shuffle(all_train_users)\n",
    "split_idx = int(len(all_train_users) * 0.8)\n",
    "train_user_ids, val_user_ids = all_train_users[:split_idx], all_train_users[split_idx:]\n",
    "\n",
    "train_part = train_df[train_df['buyer_admin_id'].isin(train_user_ids)]\n",
    "val_part = train_df[train_df['buyer_admin_id'].isin(val_user_ids)]\n",
    "\n",
    "def generate_candidates_and_features(data, is_training=True):\n",
    "    # 1. 召回\n",
    "    # 获取每个用户的历史和最后一次购买\n",
    "    last_items = data.sort_values('create_order_time').drop_duplicates('buyer_admin_id', keep='last')\n",
    "    user_last_item_map = dict(zip(last_items['buyer_admin_id'], last_items['item_id']))\n",
    "    \n",
    "    candidate_pairs = []\n",
    "    for user_id, user_group in tqdm(data.groupby('buyer_admin_id'), desc=\"Generating Candidates\"):\n",
    "        # a. 历史购买过的\n",
    "        candidates = set(user_group['item_id'].unique())\n",
    "        # b. Item-CF召回\n",
    "        last_item = user_last_item_map.get(user_id)\n",
    "        if last_item and last_item in item_cf_dict:\n",
    "            related_items = sorted(item_cf_dict[last_item].items(), key=lambda x: x[1], reverse=True)[:50]\n",
    "            candidates.update([item for item, score in related_items])\n",
    "        # c. 全局热门补充\n",
    "        candidates.update(global_top_items)\n",
    "        \n",
    "        for item_id in list(candidates)[:300]:\n",
    "            candidate_pairs.append((user_id, item_id))\n",
    "    \n",
    "    # 构建候选集DataFrame\n",
    "    candidates_df = pd.DataFrame(candidate_pairs, columns=['buyer_admin_id', 'item_id'])\n",
    "    \n",
    "    # 2. 构造标签\n",
    "    if is_training:\n",
    "        # 在训练/验证模式下，我们知道真实的目标\n",
    "        targets = data[data['irank'] == 1][['buyer_admin_id', 'item_id']]\n",
    "        targets['label'] = 1\n",
    "        candidates_df = pd.merge(candidates_df, targets, on=['buyer_admin_id', 'item_id'], how='left')\n",
    "        candidates_df['label'].fillna(0, inplace=True)\n",
    "    \n",
    "    # 3. 特征工程 (向量化)\n",
    "    print(\"Vectorized Feature Engineering...\")\n",
    "    # a. 合并商品侧特征\n",
    "    candidates_df = pd.merge(candidates_df, item_features, on='item_id', how='left')\n",
    "    # b. 合并用户侧特征\n",
    "    candidates_df = pd.merge(candidates_df, user_features, on='buyer_admin_id', how='left')\n",
    "    \n",
    "    # c. 交叉特征\n",
    "    # has_bought_before & times_bought\n",
    "    user_item_history = train_df.groupby(['buyer_admin_id', 'item_id']).size().reset_index(name='times_bought')\n",
    "    candidates_df = pd.merge(candidates_df, user_item_history, on=['buyer_admin_id', 'item_id'], how='left')\n",
    "    candidates_df['times_bought'].fillna(0, inplace=True)\n",
    "    candidates_df['has_bought_before'] = (candidates_df['times_bought'] > 0).astype(int)\n",
    "    \n",
    "    # seq_cf_score\n",
    "    last_items_df = last_items[['buyer_admin_id', 'item_id']].rename(columns={'item_id': 'last_item_id'})\n",
    "    candidates_df = pd.merge(candidates_df, last_items_df, on='buyer_admin_id', how='left')\n",
    "    \n",
    "    def cf_score_lookup(row):\n",
    "        return item_cf_dict.get(row['last_item_id'], {}).get(row['item_id'], 0)\n",
    "    \n",
    "    # apply虽然是循环，但比Python外层循环快得多，且数据量已大大减少\n",
    "    candidates_df['seq_cf_score'] = candidates_df.apply(cf_score_lookup, axis=1)\n",
    "    \n",
    "    candidates_df.drop(columns=['last_item_id'], inplace=True)\n",
    "    candidates_df.fillna(0, inplace=True)\n",
    "    \n",
    "    return candidates_df\n",
    "\n",
    "# 构建训练和验证数据集\n",
    "train_data = generate_candidates_and_features(train_part, is_training=True)\n",
    "val_data = generate_candidates_and_features(val_part, is_training=True) # 验证集也需要标签\n",
    "\n",
    "print(f\"Dataset creation finished in {(time.time() - start_time)/60:.2f} minutes.\")\n",
    "\n",
    "\n",
    "# --- 3. 模型训练 (保持不变) ---\n",
    "print(\"\\n--- 3. Model Training (LGBMRanker) ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "train_groups = train_data.groupby('buyer_admin_id').size().to_list()\n",
    "train_features = train_data.drop(columns=['buyer_admin_id', 'item_id', 'label', 'cate_id', 'store_id'])\n",
    "train_labels = train_data['label']\n",
    "\n",
    "val_groups = val_data.groupby('buyer_admin_id').size().to_list()\n",
    "val_features = val_data.drop(columns=['buyer_admin_id', 'item_id', 'label', 'cate_id', 'store_id'])\n",
    "val_labels = val_data['label']\n",
    "\n",
    "ranker = lgb.LGBMRanker(\n",
    "    objective=\"lambdarank\", metric=\"mrr\", n_estimators=500, learning_rate=0.05,\n",
    "    num_leaves=63, verbose=-1, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "ranker.fit(\n",
    "    train_features, train_labels, group=train_groups,\n",
    "    eval_set=[(val_features, val_labels)], eval_group=[val_groups],\n",
    "    eval_at=[30], callbacks=[lgb.early_stopping(20, verbose=True)]\n",
    ")\n",
    "print(f\"Model training finished in {(time.time() - start_time)/60:.2f} minutes.\")\n",
    "\n",
    "\n",
    "# --- 4. 预测与生成提交文件 (优化) ---\n",
    "print(\"\\n--- 4. Prediction and Submission ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# 使用与训练过程完全相同的逻辑来构建测试集的候选和特征\n",
    "test_data = generate_candidates_and_features(test_df, is_training=False)\n",
    "\n",
    "# 预测分数\n",
    "test_features = test_data.drop(columns=['buyer_admin_id', 'item_id', 'cate_id', 'store_id'])\n",
    "test_data['score'] = ranker.predict(test_features)\n",
    "\n",
    "# 排序并生成提交\n",
    "submission_df = test_data.sort_values(['buyer_admin_id', 'score'], ascending=[True, False])\n",
    "submission_df = submission_df.groupby('buyer_admin_id').head(30)\n",
    "submission_pivot = submission_df.pivot_table(index='buyer_admin_id', columns=submission_df.groupby('buyer_admin_id').cumcount()+1, values='item_id').reset_index()\n",
    "submission_pivot.columns = ['buyer_admin_id'] + [f'predict {i}' for i in range(1, 31)]\n",
    "\n",
    "submission_filename = 'lgbm_ranker_submission_optimized.csv'\n",
    "submission_pivot.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"Submission file created in {(time.time() - start_time)/60:.2f} minutes.\")\n",
    "print(f\"Submission file '{submission_filename}' created successfully!\")\n",
    "print(\"Preview:\")\n",
    "print(submission_pivot.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
